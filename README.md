# newsRec

新闻推荐的小项目, 原始数据取自天池大数据竞赛"零基础入门--新闻推荐", 忽略冷启动, 这里仅考虑有行为的用户和文章, 原始数据存在一定的**规律**, 即存在大量点击差为固定值30s的行为, 推测该比赛是一个**以模型逼近模型**的比赛, 为了数据集分布的合理性, 这里仅采用**最后一次点击和倒数第二次点击差为30s**的数据进行训练,并作了少许加工：

1. 筛选符合条件的数据
2. 取所有用户的最后一次点击作为正样本, 剔除点击次数≤1的用户(筛选发现所有满足1.的数据都至少有2次点击)
3. 按照规则负采样生成负样本(正/负 = 1/4)
4. 重新随机划分训练集&验证集(150000/50000)

文件构成：
1. recall  
    包含采用的召回模型, 召回的数据, 以及模型评估;  
    评估规则：topk命中率

2. sort  
    包含采用的精排模型, 以及模型评估;  
    评估规则：auc

3. data  
    原始数据预处理后的数据

4. feature  
   生成的user&item特征

5. data_preprocess  
    原始数据预处理
 
流程: 预处理 -> 特征工程 -> 负采样生成训练集 -> 召回 -> 精排  
运行方式(每一步请进入文件所在的文件夹下执行, 否则会报路径错误), 下载原始数据到 "./data_preprocess/data/"
1. 运行data_preprocess文件夹下的run.ipynb, 完成数据的预处理, 产出在当前目录的data文件夹下
2. 依次运行 feature_gen.ipynb, feature_joiner.ipynb 完成特征生成和拼接的任务, 保存在feature文件夹下
3. 运行recall文件夹下的run.ipynb完成召回, 结果保存在 recall/recall_data/下
4. 运行sort文件夹下的run.ipynb完成精排模型训练, 以及推荐序列的生成  
[原始数据下载链接](https://tianchi.aliyun.com/competition/entrance/531842/information)

#
# 特征工程
## 统一保存为csv格式
0. 预处理  
    离散特征重新label encoder;  
    数据集是基于用户的推荐, 在时间维度上不统一, 因此这里的时间特征以每一个用户最后一次点击时间为基准T, 以T-3h, T-12h, T-24h分桶.

1. 统计特征为用户的最近1,3,5,7次点


#
# 部分召回结果展示
## FM召回, 样本达300时的覆盖率
[2022-02-19 23:10:41.408552]k = 1, hit rate: 186/50000 = 0.37%  
[2022-02-19 23:10:49.377339]k = 5, hit rate: 809/50000 = 1.62%  
[2022-02-19 23:10:57.212678]k = 10, hit rate: 1525/50000 = 3.05%  
[2022-02-19 23:11:04.273601]k = 20, hit rate: 2952/50000 = 5.90%  
[2022-02-19 23:11:12.604879]k = 30, hit rate: 4363/50000 = 8.73%  
[2022-02-19 23:11:21.777417]k = 50, hit rate: 6873/50000 = 13.75%  
[2022-02-19 23:11:31.667860]k = 100, hit rate: 11656/50000 = 23.31%  
[2022-02-19 23:11:44.759606]k = 200, hit rate: 18124/50000 = 36.25%  
[2022-02-19 23:11:58.175781]k = 300, hit rate: 22469/50000 = 44.94%   
    
#
# 部分精排结果展示
## 单隐层DNN，训练的logloss & auc变化情况  
[2022-02-24 19:40:58.658768][epoch:1||train_loss/test_loss:1.0265/1.0275||train_auc/test_auc:0.895/0.892]  
[2022-02-24 19:41:43.028802][epoch:2||train_loss/test_loss:0.9911/0.9923||train_auc/test_auc:0.915/0.912]  
[2022-02-24 19:42:27.417863][epoch:3||train_loss/test_loss:0.9648/0.9661||train_auc/test_auc:0.923/0.919]  
[2022-02-24 19:43:10.879706][epoch:4||train_loss/test_loss:0.9449/0.9462||train_auc/test_auc:0.926/0.922]  
[2022-02-24 19:43:54.438555][epoch:5||train_loss/test_loss:0.9293/0.9307||train_auc/test_auc:0.927/0.923]  

## 逻辑回归，训练的logloss & auc变化情况  
[2022-02-24 19:46:32.734700][epoch:1||train_loss/test_loss:0.9743/0.9759||train_auc/test_auc:0.848/0.844]  
[2022-02-24 19:47:14.294110][epoch:2||train_loss/test_loss:0.9408/0.9430||train_auc/test_auc:0.856/0.852]  
[2022-02-24 19:47:56.897780][epoch:3||train_loss/test_loss:0.9212/0.9238||train_auc/test_auc:0.857/0.853]  
[2022-02-24 19:48:38.902293][epoch:4||train_loss/test_loss:0.9077/0.9105||train_auc/test_auc:0.857/0.853]  
[2022-02-24 19:49:23.753459][epoch:5||train_loss/test_loss:0.8975/0.9005||train_auc/test_auc:0.856/0.852]  
